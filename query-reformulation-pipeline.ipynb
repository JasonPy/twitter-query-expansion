{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Query Reformulation \n",
    "Step by step building a custom pipeline to handle queries for Twitter database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from spacy import displacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download one of the predefined German models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download de_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select german package\n",
    "MODEL = 'de_core_news_lg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load german language model\n",
    "nlp = spacy.load(MODEL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a user query to test the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Ist die große Koalition gescheitert unter Merkel? #Groko#SPD#CDU\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Tokens from SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ist', 'die', 'große', 'Koalition', 'gescheitert', 'unter', 'Merkel', '?', '#', 'Groko#SPD#CDU']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(QUERY)\n",
    "\n",
    "# displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "print([token.text for token in doc])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Preprocessing\n",
    "Hashtags are treated poorly. Detect them and prevent the tokenizer from splitting them.\n",
    "- don't split hashtag and it's text\n",
    "- split compound hashtags\n",
    "- mark hashtags in SpaCy\n",
    "\n",
    "The user mentions are kept as one token. \n",
    "- mark them as well"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tokenizer\n",
    "Modify the tokenizer such that hashtags are not split at `#`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import _get_regex_pattern\n",
    "import re\n",
    "\n",
    "# get default pattern for tokens that don't get split\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "\n",
    "# add your patterns (here: hashtags and in-word hyphens)\n",
    "re_token_match = f\"({re_token_match}|#\\w+|\\w+-\\w+)\"\n",
    "\n",
    "# overwrite token_match function of the tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ist', 'die', 'große', 'Koalition', 'gescheitert', 'unter', 'Merkel', '?', '#Groko#SPD#CDU']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in nlp(QUERY)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make sure the whitespaces are set correctly in between the hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ist', 'die', 'große', 'Koalition', 'gescheitert', 'unter', 'Merkel', '?', '#Groko', '#SPD', '#CDU']\n"
     ]
    }
   ],
   "source": [
    "def seperate_hashtags(text: str):\n",
    "    \"\"\"\n",
    "    Insert a whitespace if hashtags are missing a gap in between.  \n",
    "    \"\"\"\n",
    "    for i, j in enumerate(text):\n",
    "        if (text[i] == \"#\" and i > 0):\n",
    "            if text[i-1] != \" \":\n",
    "                    text = text[:i] + \" \" + text[i:]\n",
    "                    i+=1\n",
    "    return text\n",
    "\n",
    "QUERY = seperate_hashtags(QUERY)\n",
    "\n",
    "print([token.text for token in nlp(QUERY)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Matcher\n",
    "Customize the Matcher to handle Tweet-specific syntax - i.e. hashtags.\n",
    "- Mark Hashtag (#)\n",
    "- Mark Twitter User (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.user_matcher.UserMatcher at 0x7fca2c13a8f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.hashtag_matcher import create_hashtag_matcher\n",
    "from src.user_matcher import create_user_matcher\n",
    "\n",
    "nlp.add_pipe(\"hashtag_matcher\") \n",
    "nlp.add_pipe(\"user_matcher\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>is_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ist</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>große</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koalition</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gescheitert</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unter</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merkel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Groko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#SPD</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#CDU</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Text  is_hashtag\n",
       "0           Ist       False\n",
       "1           die       False\n",
       "2         große       False\n",
       "3     Koalition       False\n",
       "4   gescheitert       False\n",
       "5         unter       False\n",
       "6        Merkel       False\n",
       "7             ?       False\n",
       "8        #Groko        True\n",
       "9          #SPD        True\n",
       "10         #CDU        True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(QUERY)\n",
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    data.append([token, token._.is_hashtag])\n",
    "pd.DataFrame(data, columns=[\"Text\", \"is_hashtag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>is_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ist</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>große</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koalition</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gescheitert</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unter</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merkel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>?</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Groko</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#SPD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#CDU</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Text  is_user\n",
       "0           Ist    False\n",
       "1           die    False\n",
       "2         große    False\n",
       "3     Koalition    False\n",
       "4   gescheitert    False\n",
       "5         unter    False\n",
       "6        Merkel    False\n",
       "7             ?    False\n",
       "8        #Groko    False\n",
       "9          #SPD    False\n",
       "10         #CDU    False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    data.append([token, token._.is_user])\n",
    "pd.DataFrame(data, columns=[\"Text\", \"is_user\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 Named Entities\n",
    "How are named entities detected? Especially those that are hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>NER Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, NER Label]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(QUERY)\n",
    "data = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    data.append([ent.text, spacy.explain(ent.label_)])\n",
    "    \n",
    "# displacy.render(doc, style=\"ent\")\n",
    "pd.DataFrame(data, columns=[\"Text\", \"NER Label\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that named entities are treated not optimally. Sometimes named entities aren't detected or the corresponding tokens don't make sense. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>UPOS Tag</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Syntactics</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Alpha Token</th>\n",
       "      <th>Stop Token</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ist</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VAFIN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>Xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die</td>\n",
       "      <td>DET</td>\n",
       "      <td>ART</td>\n",
       "      <td>nk</td>\n",
       "      <td>xxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>große</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJA</td>\n",
       "      <td>nk</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koalition</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>sb</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gescheitert</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VVFIN</td>\n",
       "      <td>pd</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unter</td>\n",
       "      <td>ADP</td>\n",
       "      <td>APPR</td>\n",
       "      <td>mo</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Merkel</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NE</td>\n",
       "      <td>nk</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>?</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>$.</td>\n",
       "      <td>punct</td>\n",
       "      <td>?</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Groko</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NE</td>\n",
       "      <td>nk</td>\n",
       "      <td>#Xxxxx</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#SPD</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>#XXX</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#CDU</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NE</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>#XXX</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Text UPOS Tag    Tag Syntactics   Shape  Alpha Token  Stop Token  \\\n",
       "0           Ist      AUX  VAFIN       ROOT     Xxx         True        True   \n",
       "1           die      DET    ART         nk     xxx         True        True   \n",
       "2         große      ADJ   ADJA         nk    xxxx         True        True   \n",
       "3     Koalition     NOUN     NN         sb   Xxxxx         True       False   \n",
       "4   gescheitert     VERB  VVFIN         pd    xxxx         True       False   \n",
       "5         unter      ADP   APPR         mo    xxxx         True        True   \n",
       "6        Merkel    PROPN     NE         nk   Xxxxx         True       False   \n",
       "7             ?    PUNCT     $.      punct       ?        False       False   \n",
       "8        #Groko    PROPN     NE         nk  #Xxxxx        False       False   \n",
       "9          #SPD     NOUN     NN       ROOT    #XXX        False       False   \n",
       "10         #CDU    PROPN     NE       ROOT    #XXX        False       False   \n",
       "\n",
       "    Hashtag   User  \n",
       "0     False  False  \n",
       "1     False  False  \n",
       "2     False  False  \n",
       "3     False  False  \n",
       "4     False  False  \n",
       "5     False  False  \n",
       "6     False  False  \n",
       "7     False  False  \n",
       "8      True  False  \n",
       "9      True  False  \n",
       "10     True  False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    data.append ([token.text, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop, token._.is_hashtag, token._.is_user])\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Text\", \"UPOS Tag\", \"Tag\", \"Syntactics\", \"Shape\", \"Alpha Token\", \"Stop Token\", \"Hashtag\", \"User\"], index=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Candidate Selection\n",
    "Extract terms that are used to find synonyms. The words to find synonyms for should be:\n",
    "- verbs or nouns\n",
    "- no hashtags or users\n",
    "- only alphabet characters\n",
    "- no e-mail, URLs or currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_candidate_terms(doc: spacy.tokens.doc.Doc, pos_tags):\n",
    "    \"\"\"\n",
    "    Select the tokens that should be used for finding similar terms.\n",
    "    \"\"\"\n",
    "    candidate_terms = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ not in pos_tags:\n",
    "            continue\n",
    "\n",
    "        if token._.is_hashtag is True:\n",
    "            continue\n",
    "\n",
    "        if token._.is_user is True:\n",
    "            continue\n",
    "\n",
    "        if token.is_alpha is False:\n",
    "            continue\n",
    "\n",
    "        if token.like_email:\n",
    "            continue\n",
    "\n",
    "        if token.like_url:\n",
    "            continue\n",
    "\n",
    "        if token.is_currency:\n",
    "            continue\n",
    "\n",
    "        # TODO: lemmatize token ?\n",
    "        candidate_terms.append(token.text)\n",
    "    \n",
    "    return candidate_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['große', 'Koalition', 'gescheitert', 'Merkel']\n"
     ]
    }
   ],
   "source": [
    "pos_tags = [\"VERB\", \"NOUN\", \"PROPN\", \"ADJ\"]\n",
    "candidate_terms = select_candidate_terms(doc, pos_tags)\n",
    "\n",
    "print(candidate_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Word Embeddings\n",
    "The following embeddings are applied to the selected terms\n",
    "- FastText\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of most similar words (synonyms) \n",
    "NUM_SIM_TERMS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 FastText"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load FastText model with **FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download german model from fasttext website\n",
    "# !wget -P ./data/fasttext https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the fasttext model\n",
    "# !gunzip -d ./data/fasttext/cc.de.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "ft_model = fasttext.load_model('data/fasttext/cc.de.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'große': ['größere', 'grosse', 'riesengroße'], 'Koalition': ['Regierungskoalition', 'Koalitionsrunde', 'Koalitionspartei'], 'gescheitert': ['scheitert', 'Gescheitert', 'gescheitert.'], 'Merkel': ['Kanzlerin', 'Merkels', 'Bundeskanzlerin']}\n"
     ]
    }
   ],
   "source": [
    "ft_synonyms = {}\n",
    "\n",
    "# obtain candidate terms and store them in a json object\n",
    "for term in candidate_terms:\n",
    "    synonyms = ft_model.get_nearest_neighbors(term, k=NUM_SIM_TERMS)\n",
    "    ft_synonyms[f\"{term}\"] = [n[1] for n in synonyms]\n",
    "    \n",
    "print(ft_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastText module gives pretty fancy results. Even out-of-vocabulary words are treated well as expected.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Word2Vec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Word2Vec model via **Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download german model from devmount website\n",
    "!wget -P ./data/fasttext https://cloud.devmount.de/d2bc5672c523b086/german.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "gensim_w2v_model = KeyedVectors.load_word2vec_format(fname=\"data/word2vec/german.model\", no_header=False, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'groß' does not appear in this model\n",
      "{'Koalition': ['Grosse Koalition', 'Grossen Koalition', 'Regierungskoalition'], 'scheitern': ['gescheitert', 'scheitert', 'platzen'], 'Merkel': ['Kanzlerin Merkel', 'Merkel CDU', 'Bundeskanzlerin']}\n"
     ]
    }
   ],
   "source": [
    "w2v_synonyms = {}\n",
    "\n",
    "# obtain candidate terms and store them in a json object\n",
    "for term in candidate_terms:\n",
    "    if not gensim_w2v_model.has_index_for(term):\n",
    "        print(f\"The word '{term}' does not appear in this model\")\n",
    "    else:\n",
    "        synonyms = gensim_w2v_model.most_similar(term)[:NUM_SIM_TERMS]\n",
    "        w2v_synonyms[f\"{term}\"] = [n[0].replace(\"_\",\" \") for n in synonyms]\n",
    "\n",
    "print(w2v_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gensim_w2v_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model seems to work properly. However, it is case-sensitive and requires to lemmatize the terms. Otherwise the model can't find the correct word vector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Elastic Search\n",
    "\n",
    "Finally, the obtained terms are used to retrieve Tweets from the Elastic Search index. Beforehand, the most relevant expansion terms must be determined. For this purpose, the [Adjacency Matrix Aggregations](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-adjacency-matrix-aggregation.html) are utilized. Afterwards, the hashtags, twitter users and entities are prepared. Given the final expansion terms, the Elastic Search template is loaded and the query is executed on the specified `INDEX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Elastic Search...\n",
      "Successfully connected to https://localhost:9200\n"
     ]
    }
   ],
   "source": [
    "from src.utils import es_connect\n",
    "\n",
    "import json\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('auth/es-credentials.ini')\n",
    "\n",
    "es_client = es_connect(credentials=config[\"ELASTIC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the Elastic Search index \n",
    "INDEX = \"tweets\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1 Aggregation Query\n",
    "Now, it must be determined which of the terms of the initial query should be replaced or used to expand the query. For this purpose the co-occurrence of the expansion terms as well as initial terms are investigated. Terms that occur often together might be suitable expansions for the final query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predefined aggregation query\n",
    "es_agg_query = json.load(open('config/es-adjacency-matrix.conf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: don't want to blow up my memory by loading embeddings, thus take the copied output here\n",
    "ft_synonyms = {'große': ['größere', 'grosse', 'riesengroße'], 'Koalition': ['Regierungskoalition', 'Koalitionsrunde', 'Koalitionspartei'], 'gescheitert': ['scheitert', 'Gescheitert', 'gescheitert.'], 'Merkel': ['Kanzlerin', 'Merkels', 'Bundeskanzlerin']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'große': {'term': {'txt': 'große'}}, 'größere': {'term': {'txt': 'größere'}}, 'grosse': {'term': {'txt': 'grosse'}}, 'riesengroße': {'term': {'txt': 'riesengroße'}}, 'Koalition': {'term': {'txt': 'koalition'}}, 'Regierungskoalition': {'term': {'txt': 'regierungskoalition'}}, 'Koalitionsrunde': {'term': {'txt': 'koalitionsrunde'}}, 'Koalitionspartei': {'term': {'txt': 'koalitionspartei'}}, 'gescheitert': {'term': {'txt': 'gescheitert'}}, 'scheitert': {'term': {'txt': 'scheitert'}}, 'Gescheitert': {'term': {'txt': 'gescheitert'}}, 'gescheitert.': {'term': {'txt': 'gescheitert.'}}, 'Merkel': {'term': {'txt': 'merkel'}}, 'Kanzlerin': {'term': {'txt': 'kanzlerin'}}, 'Merkels': {'term': {'txt': 'merkels'}}, 'Bundeskanzlerin': {'term': {'txt': 'bundeskanzlerin'}}}\n"
     ]
    }
   ],
   "source": [
    "filters = es_agg_query[\"aggs\"][\"interactions\"][\"adjacency_matrix\"][\"filters\"]\n",
    "\n",
    "# compose the aggregation query with the candidate terms\n",
    "for term in candidate_terms:\n",
    "    filters[term] = { \"term\" : { \"txt\" : term.lower() }}\n",
    "\n",
    "    for synonym in ft_synonyms[term]:\n",
    "        filters[synonym] = { \"term\" : { \"txt\" : synonym.lower() }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3 ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term Aggregation</th>\n",
       "      <th>Document Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merkel</td>\n",
       "      <td>2908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Koalition</td>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kanzlerin</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gescheitert</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gescheitert&amp;gescheitert</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gescheitert</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bundeskanzlerin</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kanzlerin&amp;Merkel</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bundeskanzlerin&amp;Merkel</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scheitert</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Regierungskoalition</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Koalition&amp;Merkel</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gescheitert&amp;Merkel</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Merkel&amp;gescheitert</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gescheitert&amp;Koalition</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Koalition&amp;gescheitert</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bundeskanzlerin&amp;Koalition</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bundeskanzlerin&amp;Gescheitert</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bundeskanzlerin&amp;gescheitert</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Bundeskanzlerin&amp;Kanzlerin</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Kanzlerin&amp;Koalition</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Gescheitert&amp;scheitert</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gescheitert&amp;scheitert</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Koalitionspartei</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Koalition&amp;Regierungskoalition</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Koalition&amp;scheitert</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Merkel&amp;scheitert</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bundeskanzlerin&amp;scheitert</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Kanzlerin&amp;scheitert</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Term Aggregation  Document Count\n",
       "0                          Merkel            2908\n",
       "1                       Koalition            2516\n",
       "2                       Kanzlerin             906\n",
       "3                     Gescheitert             514\n",
       "4         Gescheitert&gescheitert             514\n",
       "5                     gescheitert             514\n",
       "6                 Bundeskanzlerin             486\n",
       "7                Kanzlerin&Merkel             394\n",
       "8          Bundeskanzlerin&Merkel             225\n",
       "9                       scheitert             176\n",
       "10            Regierungskoalition              80\n",
       "11               Koalition&Merkel              35\n",
       "12             Gescheitert&Merkel              24\n",
       "13             Merkel&gescheitert              24\n",
       "14          Gescheitert&Koalition              21\n",
       "15          Koalition&gescheitert              21\n",
       "16      Bundeskanzlerin&Koalition              17\n",
       "17    Bundeskanzlerin&Gescheitert              14\n",
       "18    Bundeskanzlerin&gescheitert              14\n",
       "19      Bundeskanzlerin&Kanzlerin              11\n",
       "20            Kanzlerin&Koalition               9\n",
       "21          Gescheitert&scheitert               4\n",
       "22          gescheitert&scheitert               4\n",
       "23               Koalitionspartei               3\n",
       "24  Koalition&Regierungskoalition               2\n",
       "25            Koalition&scheitert               2\n",
       "26               Merkel&scheitert               2\n",
       "27      Bundeskanzlerin&scheitert               1\n",
       "28            Kanzlerin&scheitert               1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the search aggregation query\n",
    "res = es_client.search(index=INDEX, size=es_agg_query[\"size\"], aggregations=es_agg_query[\"aggs\"])\n",
    "\n",
    "# get the aggregations and their score from the response\n",
    "aggregations = [(t[\"key\"], t[\"doc_count\"]) for t in res[\"aggregations\"][\"interactions\"][\"buckets\"]]\n",
    "\n",
    "# sort the aggregations based on their score\n",
    "aggregations.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "# print the results\n",
    "print(\"Took\",res[\"took\"],\"ms\\n\")\n",
    "pd.DataFrame(aggregations, columns=[\"Term Aggregation\", \"Document Count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 100\n",
    "\n",
    "# take a set of expansion terms (no duplicates)\n",
    "expansion_terms = set(ft_synonyms)\n",
    "\n",
    "# choose the 'best' expansion terms\n",
    "for agg in aggregations:\n",
    "    if agg[1] > ALPHA:\n",
    "        expansion_terms.update(agg[0].split(\"&\"))\n",
    "\n",
    "expansion_terms = list(expansion_terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Preparation\n",
    "Obtain a list of Hashtags, Twitter Users and Entities that are included in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#groko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#cdu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hashtag\n",
       "0  #groko\n",
       "1    #spd\n",
       "2    #cdu"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = [t.text.lower() for t in doc if t._.is_hashtag ]\n",
    "\n",
    "pd.DataFrame(hashtags, columns=[\"Hashtag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [User]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = [t.text.lower() for t in doc if t._.is_user ]\n",
    "\n",
    "pd.DataFrame(users, columns=[\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Entity]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = [ent.text.lower() for ent in doc.ents]\n",
    "\n",
    "pd.DataFrame(entities, columns=[\"Entity\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.3 Query Formulation \n",
    "Finally, the resulting terms must be arranged in an Elastic Search query. Define a pattern to retrieve relevant tweets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a pattern for an Elastic Search query with\n",
    "- boolean operators (`AND`, `OR`)\n",
    "- boosting  `^`\n",
    "- filter\n",
    "\n",
    "The following Hyperparameters are set in order to modify the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are retweets allowed?\n",
    "RETWEET = False\n",
    "\n",
    "# How much is the matching of hashtags boosted? \n",
    "HASHTAG_BOOST = 0.5\n",
    "\n",
    "# Range of Tweets to be included (FROM, TO)\n",
    "TWEET_RANGE = (\"2021-01-01\", \"2023-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-configured template for an elastic search query\n",
    "es_query = json.load(open('config/es-query.conf'))\n",
    "query = es_query['query']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set if retweets are allowed\n",
    "if RETWEET:\n",
    "    del query['bool']['must_not']['term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Koalition',\n",
       " 'Kanzlerin',\n",
       " 'Bundeskanzlerin',\n",
       " 'scheitert',\n",
       " 'große',\n",
       " 'Merkel',\n",
       " 'Gescheitert',\n",
       " 'gescheitert']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(expansion_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for testing I use candidate terms which are not yet applied to word embeddings\n",
    "query['bool']['should'][0]['match']['txt']['query'] = ' '.join(expansion_terms)\n",
    "query['bool']['should'][1]['terms']['hashtags'] = [expansion_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the hashtags if present in initial user query\n",
    "if len(hashtags) > 0 :\n",
    "    query['bool']['must']['terms_set']['hashtags']['terms'] = [h[1:].lower() for h in hashtags]\n",
    "else:\n",
    "    del query['bool']['must']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HASHTAG_BOOST is not None:\n",
    "    query['bool']['should'][1]['terms'][\"boost\"] = HASHTAG_BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "for entity in entities:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date range for tweets\n",
    "query['bool']['filter'][0]['range']['created_at']['gte'] = TWEET_RANGE[0]\n",
    "query['bool']['filter'][1]['range']['created_at']['lte'] = TWEET_RANGE[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bool': {'should': [{'match': {'txt': {'query': 'Koalition Kanzlerin Bundeskanzlerin scheitert große Merkel Gescheitert gescheitert',\n",
       "      'operator': 'OR'}}},\n",
       "   {'terms': {'hashtags': {'Bundeskanzlerin',\n",
       "      'Gescheitert',\n",
       "      'Kanzlerin',\n",
       "      'Koalition',\n",
       "      'Merkel',\n",
       "      'gescheitert',\n",
       "      'große',\n",
       "      'scheitert'},\n",
       "     'boost': 0.5}}],\n",
       "  'must': {'terms_set': {'hashtags': {'terms': ['groko', 'spd', 'cdu'],\n",
       "     'minimum_should_match_script': {'source': 'Math.min(params.num_terms, 1)'}}}},\n",
       "  'must_not': {'term': {'txt': '_retweet_'}},\n",
       "  'filter': [{'range': {'created_at': {'gte': '2021-01-01'}}},\n",
       "   {'range': {'created_at': {'lte': '2023-01-01'}}}]}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Query\n",
    "es_query[\"query\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the final Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SerializationError",
     "evalue": "Unable to serialize to JSON: {'aggregations': {'sample': {'sampler': {'shard_size': 500}, 'aggs': {'keywords': {'significant_terms': {'field': 'hashtags'}}}}}, 'query': {'bool': {'should': [{'match': {'txt': {'query': 'Koalition Kanzlerin Bundeskanzlerin scheitert große Merkel Gescheitert gescheitert', 'operator': 'OR'}}}, {'terms': {'hashtags': {'Koalition', 'Kanzlerin', 'Bundeskanzlerin', 'scheitert', 'große', 'Merkel', 'Gescheitert', 'gescheitert'}, 'boost': 0.5}}], 'must': {'terms_set': {'hashtags': {'terms': ['groko', 'spd', 'cdu'], 'minimum_should_match_script': {'source': 'Math.min(params.num_terms, 1)'}}}}, 'must_not': {'term': {'txt': '_retweet_'}}, 'filter': [{'range': {'created_at': {'gte': '2021-01-01'}}}, {'range': {'created_at': {'lte': '2023-01-01'}}}]}}, 'size': 10} (type: dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elastic_transport/_serializer.py:108\u001b[0m, in \u001b[0;36mJsonSerializer.dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjson_dumps(data)\n\u001b[1;32m    109\u001b[0m \u001b[39m# This should be captured by the .default()\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# call but just in case we also wrap these.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elastic_transport/_serializer.py:79\u001b[0m, in \u001b[0;36mJsonSerializer.json_dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjson_dumps\u001b[39m(\u001b[39mself\u001b[39m, data: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m     80\u001b[0m         data, default\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault, ensure_ascii\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, separators\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     81\u001b[0m     )\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[1;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[1;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m--> 238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elasticsearch/serializer.py:73\u001b[0m, in \u001b[0;36mJsonSerializer.default\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m---> 73\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to serialize \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m!r}\u001b[39;00m\u001b[39m (type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to serialize {'Koalition', 'Kanzlerin', 'Bundeskanzlerin', 'scheitert', 'große', 'Merkel', 'Gescheitert', 'gescheitert'} (type: <class 'set'>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSerializationError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m es_client\u001b[39m.\u001b[39;49msearch(index\u001b[39m=\u001b[39;49mINDEX, size\u001b[39m=\u001b[39;49mes_query[\u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m], query\u001b[39m=\u001b[39;49mes_query[\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m], aggregations\u001b[39m=\u001b[39;49mes_query[\u001b[39m\"\u001b[39;49m\u001b[39maggs\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTotal of \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m\"\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m hits in \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m\"\u001b[39m\u001b[39mtook\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39mms \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(res[\u001b[39m\"\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mhits\u001b[39m\u001b[39m\"\u001b[39m]):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:414\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m api(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elasticsearch/_sync/client/__init__.py:3770\u001b[0m, in \u001b[0;36mElasticsearch.search\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, from_, highlight, human, ignore_throttled, ignore_unavailable, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, request_cache, rescore, rest_total_hits_as_int, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version)\u001b[0m\n\u001b[1;32m   3768\u001b[0m \u001b[39mif\u001b[39;00m __body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3769\u001b[0m     __headers[\u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 3770\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperform_request(  \u001b[39m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   3771\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m, __path, params\u001b[39m=\u001b[39;49m__query, headers\u001b[39m=\u001b[39;49m__headers, body\u001b[39m=\u001b[39;49m__body\n\u001b[1;32m   3772\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:286\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m     target \u001b[39m=\u001b[39m path\n\u001b[0;32m--> 286\u001b[0m meta, resp_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransport\u001b[39m.\u001b[39;49mperform_request(\n\u001b[1;32m    287\u001b[0m     method,\n\u001b[1;32m    288\u001b[0m     target,\n\u001b[1;32m    289\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest_headers,\n\u001b[1;32m    290\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    291\u001b[0m     request_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_timeout,\n\u001b[1;32m    292\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_max_retries,\n\u001b[1;32m    293\u001b[0m     retry_on_status\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_status,\n\u001b[1;32m    294\u001b[0m     retry_on_timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_on_timeout,\n\u001b[1;32m    295\u001b[0m     client_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client_meta,\n\u001b[1;32m    296\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[39m# HEAD with a 404 is returned as a normal response\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# since this is used as an 'exists' functionality.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m meta\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m) \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    301\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m meta\u001b[39m.\u001b[39mstatus \u001b[39m<\u001b[39m \u001b[39m299\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[39mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[1;32m    307\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elastic_transport/_transport.py:307\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, target, body, headers, max_retries, retry_on_status, retry_on_timeout, request_timeout, client_meta)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcontent-type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m request_headers:\n\u001b[1;32m    304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust provide a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m\u001b[39m header to requests with bodies\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         )\n\u001b[0;32m--> 307\u001b[0m     request_body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserializers\u001b[39m.\u001b[39;49mdumps(\n\u001b[1;32m    308\u001b[0m         body, mimetype\u001b[39m=\u001b[39;49mrequest_headers[\u001b[39m\"\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     request_body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elastic_transport/_serializer.py:193\u001b[0m, in \u001b[0;36mSerializerCollection.dumps\u001b[0;34m(self, data, mimetype)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdumps\u001b[39m(\u001b[39mself\u001b[39m, data: Any, mimetype: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbytes\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_serializer(mimetype)\u001b[39m.\u001b[39;49mdumps(data)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/twitter-query-expansion-tWkdo8vh/lib/python3.10/site-packages/elastic_transport/_serializer.py:112\u001b[0m, in \u001b[0;36mJsonSerializer.dumps\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# This should be captured by the .default()\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# call but just in case we also wrap these.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mUnicodeError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# pragma: nocover\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m SerializationError(\n\u001b[1;32m    113\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to serialize to JSON: \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m!r}\u001b[39;00m\u001b[39m (type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m         errors\u001b[39m=\u001b[39m(e,),\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mSerializationError\u001b[0m: Unable to serialize to JSON: {'aggregations': {'sample': {'sampler': {'shard_size': 500}, 'aggs': {'keywords': {'significant_terms': {'field': 'hashtags'}}}}}, 'query': {'bool': {'should': [{'match': {'txt': {'query': 'Koalition Kanzlerin Bundeskanzlerin scheitert große Merkel Gescheitert gescheitert', 'operator': 'OR'}}}, {'terms': {'hashtags': {'Koalition', 'Kanzlerin', 'Bundeskanzlerin', 'scheitert', 'große', 'Merkel', 'Gescheitert', 'gescheitert'}, 'boost': 0.5}}], 'must': {'terms_set': {'hashtags': {'terms': ['groko', 'spd', 'cdu'], 'minimum_should_match_script': {'source': 'Math.min(params.num_terms, 1)'}}}}, 'must_not': {'term': {'txt': '_retweet_'}}, 'filter': [{'range': {'created_at': {'gte': '2021-01-01'}}}, {'range': {'created_at': {'lte': '2023-01-01'}}}]}}, 'size': 10} (type: dict)"
     ]
    }
   ],
   "source": [
    "res = es_client.search(index=INDEX, size=es_query['size'], query=es_query[\"query\"], aggregations=es_query[\"aggs\"])\n",
    "\n",
    "print(f'Total of {res[\"hits\"][\"total\"][\"value\"]} hits in {res[\"took\"]}ms \\n')\n",
    "\n",
    "for i, doc in enumerate(res[\"hits\"][\"hits\"]):\n",
    "    print(\"Tweet\", i, \"\\n\", doc[\"_source\"], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('twitter-query-expansion-tWkdo8vh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ad3c02dd4b61e1f3ed1e38bd9b3b7a8e15a3f55cb03b1470e9f32af9138128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
