{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Query Expansion\n",
    "© Jason Pyanowski\n",
    "\n",
    "In this demo file, the application of the project **Twitter Query Expansion** is explained. Starting with the initial data retrieval and download of the Word Embedding models. Then the pipeline is invoked and the results are inspected. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    " This allows to search Tweets quickly. In order to make use of this, the Tweets must be parsed from the PostgreSQL database into an Elastic Search Index. This task is handled by the script `/scripts/tweet_feeder.py` as stated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   morphologizer     token.morph                      pos_acc            False      \n",
      "                      token.pos                        morph_acc                     \n",
      "                                                       morph_per_feat                \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "4   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "                                                                                     \n",
      "6   hashtag_matcher                                                       False      \n",
      "                                                                                     \n",
      "7   user_matcher                                                          False      \n",
      "\n",
      "\u001b[38;5;2m✔ No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'morphologizer': {'assigns': ['token.morph', 'token.pos'],\n",
       "   'requires': [],\n",
       "   'scores': ['pos_acc', 'morph_acc', 'morph_per_feat'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'hashtag_matcher': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'user_matcher': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'morphologizer': [],\n",
       "  'parser': [],\n",
       "  'lemmatizer': [],\n",
       "  'attribute_ruler': [],\n",
       "  'ner': [],\n",
       "  'hashtag_matcher': [],\n",
       "  'user_matcher': []},\n",
       " 'attrs': {'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.pos': {'assigns': ['morphologizer'], 'requires': []},\n",
       "  'token.morph': {'assigns': ['morphologizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipeline.text_processor import TextProcessor\n",
    "a = TextProcessor()\n",
    "a.nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 scripts/tweet_feeder.py -i tweet_index -t tweet_table -wc 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Models\n",
    "This Pipeline allows tu use different word embedding models. The download link of the desired model can be used to load the model below. The model types of `fasttext` and `word2vec` are currently supported. To speed up the performance of the query expansion pipeline, the models are consequently compressed.\n",
    "\n",
    "|Parameter|Possible Values|Datatype|\n",
    "|---|---|---|\n",
    "|type|`'fasttext'`, `'word2vec'`|`str`|\n",
    "|url|`'url to model'`|`str`|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.model_loader import load_model\n",
    "load_model(type=\"word2vec\", url=\"https://cloud.devmount.de/d2bc5672c523b086/german.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(type=\"fasttext\", url=\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Queries\n",
    "Specify queries on which to evaluate the pipeline. Queries may include Twitter-specific syntax like hashtags `#EU` or user mentions `@bundestag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"große Koalition gescheitert unter Merkel? #Groko #SPD #CDU\",\n",
    "    \"Lauterbach Deutschland Corona-Maßnahmen #Impfung\",\n",
    "    \"@bundestag Bundestagswahl 2021 Ergebnisse\",\n",
    "    \"EU Brexit Boris Johnson\",\n",
    "    \"Gesetzliche Rentenversicherung Rente Mit 67\",\n",
    "    \"Klimapolitik Deutschland #Grüne\",\n",
    "    \"Asylpolitik Merkel\",\n",
    "    \"Soli abschaffen Westen\",\n",
    "    \"Bundeswehr Afghanistan Krieg stoppen\",\n",
    "    \"Deutschland Energiewende mit SPD CDU\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Word Embedding Parameters\n",
    "In order to obtain the desired results, modify the parameters for Word Embeddings. These configurations determine which of the initial query terms are actually used to find related terms.\n",
    "\n",
    "| Parameter | Possible Values | Datatype |\n",
    "|---|---|---|\n",
    "|type|`'word2vec', 'fasttext'`|`str`|\n",
    "|model| `'path to model'`|`str`|\n",
    "|pos_list|`['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'CONJ', 'DET', 'EOL', 'IDS', 'INTJ', 'NAMES', 'NOUN', 'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X']`| `list[str]`|\n",
    "|entity_list|`['LOC', 'MISC', 'ORG', 'PER']`|`list[str]`|\n",
    "|hashtag|`True, False`|`bool`|\n",
    "|user|`True, False`|`bool`|\n",
    "|num_nearest_terms|`1...N`|`int`|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_PARAMS = {\n",
    "    \"type\": \"fasttext\",\n",
    "    \"model\": \"models/fasttext/cc.de.300.model\",\n",
    "    \"pos_list\": [\"NOUN\",\"ADJ\",\"VERB\",\"PROPN\"],\n",
    "    \"entity_list\": ['LOC', 'ORG'],\n",
    "    \"hashtag\": True,\n",
    "    \"user\": False,\n",
    "    \"num_nearest_terms\": 5\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Elastic Search Parameters\n",
    "After Query Expansion, the Tweets are retrieved from an Elastic Search Index. Specify the parameters below and make sure that an Index is running on your machine. \n",
    "\n",
    "| Parameter | Possible Values | Datatype |\n",
    "|---|---|---|\n",
    "|index|`'tweets'`|`str`|\n",
    "|num_of_tweets|`1...N`| `int`|\n",
    "|retweet|`True, False`|`bool`|\n",
    "|hashtag_boost|`0...N`|`float`|\n",
    "|tweet_range|`(date, date)`|`tuple`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_PARAMS = {\n",
    "    \"index\": \"tweets_all\",\n",
    "    \"num_of_tweets\": 10,\n",
    "    \"retweet\": False,\n",
    "    \"hashtag_boost\": 1.0,\n",
    "    \"tweet_range\": (\"2020-01-01\", \"2023-01-01\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Pipeline\n",
    "Run the Pipeline - the results are stored in the `/out` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text using SpaCy...\n",
      "Evaluating fasttext model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# run pipeline\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m res \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m      5\u001b[0m     queries\u001b[39m=\u001b[39;49mQUERIES, \n\u001b[1;32m      6\u001b[0m     embedding_params\u001b[39m=\u001b[39;49mEMBEDDING_PARAMS,\n\u001b[1;32m      7\u001b[0m     elastic_params\u001b[39m=\u001b[39;49mELASTIC_PARAMS)\n",
      "File \u001b[0;32m~/Projects/Practical/twitter-query-expansion/scripts/pipeline.py:74\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(queries, embedding_params, elastic_params)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# find similar terms using embedding model \u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m tokens \u001b[39min\u001b[39;00m query_tokens:\n\u001b[0;32m---> 74\u001b[0m     similar_terms\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mget_similar_terms(trim_symbols(tokens), embedding_params[\u001b[39m\"\u001b[39m\u001b[39mnum_nearest_terms\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     75\u001b[0m log[\u001b[39m\"\u001b[39m\u001b[39msimilar_terms\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m similar_terms\n\u001b[1;32m     77\u001b[0m \u001b[39m# free space\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Practical/twitter-query-expansion/pipeline/text_processor.py:135\u001b[0m, in \u001b[0;36mtrim_symbols\u001b[0;34m(tokens, symbols)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m# Iterate over the tokens in the doc\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# Check if the token begins with sym\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39;49mtext[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m symbols:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# Remove the symbol from the token text\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         text\u001b[39m.\u001b[39mappend(token\u001b[39m.\u001b[39mtext[\u001b[39m1\u001b[39m:])\n\u001b[1;32m    138\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# run pipeline\n",
    "from scripts import pipeline\n",
    "\n",
    "res = pipeline.run(\n",
    "    queries=QUERIES, \n",
    "    embedding_params=EMBEDDING_PARAMS,\n",
    "    elastic_params=ELASTIC_PARAMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "Load results and have a look through the retrieved Tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweets, i in zip(res,range(len(QUERIES))):\n",
    "    print(\"Query:\", QUERIES[i], \"\\n\")\n",
    "    for tweet in tweets[\"tweets\"]:\n",
    "        print(\"->\", tweet[\"_source\"][\"txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter-query-expansion-tWkdo8vh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ad3c02dd4b61e1f3ed1e38bd9b3b7a8e15a3f55cb03b1470e9f32af9138128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
